{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f18c034",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "## 36-615, Spring 2023\n",
    "\n",
    "TODO YOUR NAME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1fb79",
   "metadata": {},
   "source": [
    "Because of the timing, this homework isn't super computational intensive. Instead it's meant to force you to get PyTorch working and start getting familiar with how it works. Be sure to open the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and start learning how to find things you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc113f73",
   "metadata": {},
   "source": [
    "## Problem 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb2ee7",
   "metadata": {},
   "source": [
    "Let's consider implementing gradient descent on a problem we can easily optimize analytically: linear regression. This is technically pointless since you have an analytical solution. But you already understand linear regression, so it'll be easier to understand gradient descent and its pitfalls.\n",
    "\n",
    "To make things slightly more interesting, let's do *weighted* linear regression. In weighted linear regression, we solve the optimization problem\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat \\beta &= \\operatorname*{argmin}_\\beta \\text{RSS} \\\\\n",
    "&= \\operatorname*{argmin}_\\beta \\sum_{i=1}^n w_i (Y_i - \\hat Y_i)^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $w$ is a vector of weights, such that $w_i \\geq 0$ for all $i$, that are chosen or known in advance. For example, if we know the responses have unequal variance and know the variances, setting $w_i \\propto 1/\\operatorname{var}(\\epsilon_i)$ will allow us to account for the unequal variances.\n",
    "\n",
    "In this problem, we'll have two covariates and an intercept, so $\\hat Y_i = \\hat \\beta_0 + \\hat \\beta_1 X_1 + \\hat \\beta_2 X_2$. The covariates are correlated with each other. Here are the true betas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = torch.tensor(7.)\n",
    "beta = torch.tensor([-2., 2.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532ad91",
   "metadata": {},
   "source": [
    "We'll simulate 100 observations of data with normally distributed error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37827dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "X = torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(2),\n",
    "                                                               torch.tensor([[1, 0.7], [0.7, 1]])).sample((N,))\n",
    "Y = beta_0 + X @ beta + torch.normal(mean=torch.zeros(N), std=torch.tensor([1.]))\n",
    "w = torch.rand(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de864067",
   "metadata": {},
   "source": [
    "### Problem 1 (a): The Shape of the Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3caa10",
   "metadata": {},
   "source": [
    "If we fix the intercept at its true value and vary the slopes, we can plot the objective function against the two slopes using a contour plot. \n",
    "\n",
    "Fill in the code below to plot the RSS when $\\beta_0 = 7$ and we change $\\beta_1$ and $\\beta_2$ across the range given.\n",
    "\n",
    "*Hint:* The [`.item()` method](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item) can convert a `Tensor` containing a single value into an ordinary Python number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65877cd",
   "metadata": {
    "tags": [
     "prompt"
    ]
   },
   "outputs": [],
   "source": [
    "b1s = np.linspace(-8, 4)\n",
    "b2s = np.linspace(-4, 8)\n",
    "\n",
    "b1, b2 = np.meshgrid(b1s, b2s)\n",
    "\n",
    "RSS_grid = np.zeros_like(b1)\n",
    "\n",
    "for ii in range(b1.shape[0]):\n",
    "    for jj in range(b1.shape[1]):\n",
    "        beta = torch.tensor([b1[ii, jj], b2[ii, jj]], dtype=torch.float)\n",
    "        # TODO fill in:\n",
    "        RSS_grid[ii, jj] = \n",
    "        \n",
    "plt.contour(b1, b2, RSS_grid, levels=20)\n",
    "plt.xlabel(\"beta1\")\n",
    "plt.ylabel(\"beta2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa3597",
   "metadata": {},
   "source": [
    "Comment on the shape of this plot. What feature of the data causes its shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34049bb",
   "metadata": {
    "tags": [
     "prompt"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65396ef3",
   "metadata": {},
   "source": [
    "### Problem 1 (b): Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97824c19",
   "metadata": {},
   "source": [
    "The function `regression_gradient_descent()` implements gradient descent for linear regression. Given starting values for the intercept and slopes, it calculates the residual sum of squares, uses PyTorch to get its gradients, and updates the intercept and slopes accordingly.\n",
    "\n",
    "Fill in the function where indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367b14d",
   "metadata": {
    "tags": [
     "prompt"
    ]
   },
   "outputs": [],
   "source": [
    "def regression_gradient_descent(beta_0, beta, max_iter=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Run gradient descent on the linear regression model.\n",
    "    \n",
    "    - beta_0: Scalar giving the starting value for the intercept.\n",
    "    - beta: List of two entries, giving the starting values for the slopes.\n",
    "    - max_iter: Maximum number of iterations allowed.\n",
    "    - learning_rate: Learning rate used.\n",
    "    \n",
    "    Returns a (betas, errors). betas is a numpy array of beta values with two \n",
    "    columns, one per slope; errors is a list of RSS values at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    errors = []\n",
    "    betas = [beta]\n",
    "    prev_error = float(\"+inf\")\n",
    "\n",
    "    # Starting values\n",
    "    beta_0 = torch.tensor(beta_0, requires_grad=True, dtype=torch.float)\n",
    "    beta = torch.tensor(beta, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "    for ii in range(max_iter):\n",
    "        # Calculate RSS and store the value\n",
    "        RSS = # TODO fill in\n",
    "    \n",
    "        new_error = RSS.item()\n",
    "    \n",
    "        errors.append(new_error)\n",
    "    \n",
    "        # check if we have converged\n",
    "        if abs(new_error - prev_error) / prev_error < 0.0001:\n",
    "            break\n",
    "        prev_error = new_error\n",
    "    \n",
    "        # back-propagate to get gradients\n",
    "        RSS.backward()\n",
    "    \n",
    "        # Don't track gradients of these calculations -- otherwise PyTorch would\n",
    "        # try to take gradients of this math too\n",
    "        with torch.no_grad():\n",
    "            # hint: the .data property of tensors can be used to access their numerical contents\n",
    "            beta -= # TODO fill in\n",
    "            beta_0 -= # TODO fill in\n",
    "        \n",
    "            # Store this iteration's beta estimate\n",
    "            betas.append(beta.tolist())\n",
    "        \n",
    "            # Reset the gradient information to zero\n",
    "            beta.grad.zero_()\n",
    "            beta_0.grad.zero_()\n",
    "\n",
    "    return np.array(betas), errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cae54c",
   "metadata": {},
   "source": [
    "To test your function, run the following code. It runs gradient descent with particular starting values, then plots the $\\beta$ values from each iteration. The true values are marked in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1969d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas, errors = regression_gradient_descent(0., [-7., 0.])\n",
    "\n",
    "plt.contour(b1, b2, RSS_grid, levels=20)\n",
    "plt.scatter([-2.], [2.], color=\"red\") # the true minimum\n",
    "plt.plot(betas[:, 0], betas[:, 1], '-o') # from  gradient descent\n",
    "plt.xlabel(\"beta1\")\n",
    "plt.ylabel(\"beta2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57711e4",
   "metadata": {},
   "source": [
    "It's also common to plot the error over each iteration; the error should generally decrease over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(errors)), errors)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RSS\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb612e62",
   "metadata": {},
   "source": [
    "### Problem 1 (c): The Dangers of Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef70d1",
   "metadata": {},
   "source": [
    "Above, you used the default learning rate of $\\gamma = 0.001$. In vanilla gradient descent, the learning rate is a key parameter.\n",
    "\n",
    "Plot the solution path (the sequence of $\\beta$ values) on top of the contour plot, as you did in part (b), for three learning rates. Choose one learning rate so the fit converges extremely slowly, one so it converges quickly, and one so that it does not converge at all, no matter how many iterations you allow it to take.\n",
    "\n",
    "Also, for each value, plot the RSS versus iteration, as you did in part (b). Comment on the *speed* of convergence for each fit, meaning the number of iterations taken to reach a particular value of RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b03ca8",
   "metadata": {
    "tags": [
     "prompt"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcc33bbc",
   "metadata": {},
   "source": [
    "### Problem 1 (d): Using PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0d887",
   "metadata": {},
   "source": [
    "PyTorch has the idea of building *modules*. Each module is made out of layers, where a layer takes inputs, does some calculation, and produces outputs; the outputs of one layer become the inputs of another. We'll see this in action when we do neural networks, which are composed of many layers.\n",
    "\n",
    "In a neural network, a *linear layer* does the transformation $y = xA^T + b$, where $x \\in \\mathbb{R}^p$, $A \\in \\mathbb{R}^{q \\times p}$, $b \\in \\mathbb{R}^q$, and $y \\in \\mathbb{R}^q$. In neural network terms, $A$ is a matrix of *weights* and $b$ is a vector of *biases*. Note that we could have $q \\neq p$, so a linear layer takes vectors and converts them to vectors of potentially different dimension.\n",
    "\n",
    "If we want to represent a linear regression model that takes two covariates and an intercept and produces a scalar output---like the data we simulated---what values of $q$ and $p$ should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f4165",
   "metadata": {
    "tags": [
     "prompt"
    ]
   },
   "source": [
    "$q = ?$, $p = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257caf8",
   "metadata": {},
   "source": [
    "The [`torch.nn.Linear()` class](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) represents a linear layer. Figure out the correct arguments to create the linear layer we want. (We imported `torch.nn` as `nn`, so you can just use `nn.Linear`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e83c7b",
   "metadata": {
    "tags": [
     "prompt"
    ]
   },
   "outputs": [],
   "source": [
    "nn.Linear(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73863163",
   "metadata": {},
   "source": [
    "We then create an object with a bit of boilerplate. This object represents our network and does the calculations. The `forward()` method specifies how to do the forward pass of the calculations---how to take `x` and calculate `y`, in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2eb499",
   "metadata": {
    "tags": [
     "prompt"
    ]
   },
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        \n",
    "        self.linear = # TODO fill in your linear layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9b2e5",
   "metadata": {},
   "source": [
    "Next, we create an instance of the class and create an optimizer. The `SGD` optimizer does stochastic gradient descent (we'll explain \"stochastic\" soon). It does much of the work you did in (b) automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59894e87",
   "metadata": {},
   "source": [
    "Now we train the model. This is a template for how model training will look for many kinds of models in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d558b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, max_iter=10000):\n",
    "    # Storage\n",
    "    losses = []\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        # Reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Obtain the model's predictions with the data\n",
    "        predictions = model(X)\n",
    "    \n",
    "        # Get the loss, using the known Ys\n",
    "        loss = nn.functional.mse_loss(predictions[:, 0], Y)\n",
    "    \n",
    "        # Calculate the gradients with backpropagation\n",
    "        loss.backward()\n",
    "    \n",
    "        # Perform one gradient descent step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a9f18",
   "metadata": {},
   "source": [
    "Here we print out the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89458ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d1368",
   "metadata": {},
   "source": [
    "Interpret these estimates. There is a weight and a bias; write out the fit in the form of the estimated regression equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d61a7",
   "metadata": {},
   "source": [
    "$y = ? + ? x_1 + ? x_2$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
